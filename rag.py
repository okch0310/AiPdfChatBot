# ğŸ¤– ì¸ê³µì§€ëŠ¥ PDF Q&A ì±—ë´‡
import gradio as gr
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_text_splitters import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnableMap, RunnablePassthrough, RunnableLambda

# ==================== 1ï¸âƒ£ í™˜ê²½ ì„¤ì • ====================
load_dotenv()

# LLM (OpenAI GPT-4o-mini)
llm = ChatOpenAI(model="gpt-4o-mini")

# í…ìŠ¤íŠ¸ ë¶„ë¦¬ê¸°
text_splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)

# ì„ë² ë”© ëª¨ë¸
hf_embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': 'cpu'}
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ëŒ€í™” ê¸°ë¡ í¬í•¨)
system_message = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥ì„ í† ëŒ€ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ë§Œì•½ ë¬¸ë§¥ì—ì„œ ë‹µë³€ì„ ìœ„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ 
`ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.` ë¼ê³  ë‹µí•˜ì„¸ìš”.
ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤ë©´ í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

## ì´ì „ ëŒ€í™”:
{chat_history}

## ì£¼ì–´ì§„ ë¬¸ë§¥:
{context}

## ì‚¬ìš©ì ì§ˆë¬¸:
{input}"""

prompt_template = ChatPromptTemplate.from_messages([
    ("human", system_message)
])

parser = StrOutputParser()

# ì „ì—­ ë³€ìˆ˜
db = None
retriever = None
rag_chain = None
chat_history = []  # ëŒ€í™” ê¸°ë¡ ì €ì¥


# ==================== 2ï¸âƒ£ PDF ì—…ë¡œë“œ ì²˜ë¦¬ í•¨ìˆ˜ ====================
def load_pdf(file, chat_history):
    global db, retriever, rag_chain

    print("ğŸ“‚ file:", file)
    if not file:
        return chat_history, "âš ï¸ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

    try:
        # Gradio 5.xì—ì„œëŠ” íŒŒì¼ ê²½ë¡œê°€ ë¬¸ìì—´ë¡œ ì „ë‹¬ë¨
        if isinstance(file, str):
            file_path = file
        elif hasattr(file, 'path'):
            file_path = file.path
        elif hasattr(file, 'name'):
            file_path = file.name
        else:
            file_path = str(file)

        print("ğŸ“‚ file_path:", file_path)

        loader = PyMuPDFLoader(file_path)
        docs = loader.load()
        print(f"âœ… PDF í˜ì´ì§€ ìˆ˜: {len(docs)}")

        docs = text_splitter.split_documents(docs)
        db = FAISS.from_documents(docs, hf_embeddings)
        retriever = db.as_retriever(search_kwargs={"k": 3})

        # Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        # inputì—ì„œ ì§ˆë¬¸ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        def get_question(input_dict):
            return input_dict["input"] if isinstance(input_dict, dict) else input_dict

        rag_chain = (
            RunnableMap({
                "context": RunnableLambda(get_question) | retriever | RunnableLambda(format_docs),
                "input": RunnablePassthrough(),
                "chat_history": RunnablePassthrough(),
            })
            | prompt_template
            | llm
            | parser
        )

        print("âœ… RAG chain ìƒì„± ì™„ë£Œ")
        status_msg = "âœ… PDF íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œ ë° ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!"
        # ì±„íŒ…ì— ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
        chat_history.append(("", status_msg))
        return chat_history, status_msg

    except Exception as e:
        print("âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:", e)
        import traceback
        traceback.print_exc()
        error_msg = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        return chat_history, error_msg


# ==================== 3ï¸âƒ£ ì§ˆë¬¸ ì‘ë‹µ ì²˜ë¦¬ í•¨ìˆ˜ (ì±„íŒ… ì¸í„°í˜ì´ìŠ¤) ====================
def add_message(message, history):
    """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ…ì°½ì— ì¦‰ì‹œ ì¶”ê°€"""
    if not message or not message.strip():
        return history, ""
    history.append((message, "ğŸ’­ ë‹µë³€ ìƒì„± ì¤‘..."))
    return history, ""


def chat_with_pdf(history):
    """PDF ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
    global rag_chain
    
    if not history:
        return history
    
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€(ì‚¬ìš©ì ì§ˆë¬¸)ë¥¼ ê°€ì ¸ì˜´
    current_question = history[-1][0]
    
    if rag_chain is None:
        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ ë‹µë³€ ë¶€ë¶„ë§Œ ì—…ë°ì´íŠ¸
        history[-1] = (current_question, "âš ï¸ PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return history
    
    if not current_question or not current_question.strip():
        return history
    
    try:
        # ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸)
        chat_history_str = ""
        if len(history) > 1:
            for human_msg, ai_msg in history[:-1]:
                if human_msg:
                    chat_history_str += f"ì‚¬ìš©ì: {human_msg}\n"
                if ai_msg and not ai_msg.startswith("ğŸ’­"):
                    chat_history_str += f"AI: {ai_msg}\n"
        
        # í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        response = rag_chain.invoke({
            "input": current_question,
            "chat_history": chat_history_str if chat_history_str else "ì´ì „ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤."
        })
        
        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ ë‹µë³€ ë¶€ë¶„ë§Œ ì—…ë°ì´íŠ¸
        history[-1] = (current_question, response)
        return history
    
    except Exception as e:
        print("âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:", e)
        import traceback
        traceback.print_exc()
        error_msg = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        history[-1] = (current_question, error_msg)
        return history


# ==================== 4ï¸âƒ£ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” í•¨ìˆ˜ ====================
def clear_chat():
    global chat_history
    chat_history = []
    return []


# ==================== 5ï¸âƒ£ Gradio UI êµ¬ì„± (ì±„íŒ… ì¸í„°í˜ì´ìŠ¤) ====================
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ¤– AI PDF Q&A ì±—ë´‡
    **PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì±„íŒ…ìœ¼ë¡œ ì§ˆë¬¸í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤!**
    """)

    with gr.Row():
        with gr.Column(scale=1):
            file_input = gr.File(label="ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ", file_types=[".pdf"])
            upload_button = gr.Button("ğŸ“¥ ì—…ë¡œë“œ ë° ì²˜ë¦¬", variant="primary")
            status_output = gr.Textbox(label="ìƒíƒœ", interactive=False, lines=2)

        with gr.Column(scale=2):
            chatbot = gr.Chatbot(
                label="ğŸ’¬ ì±„íŒ…",
                height=500,
                show_copy_button=True,
                avatar_images=(None, "ğŸ¤–")
            )
            with gr.Row():
                msg = gr.Textbox(
                    label="ì§ˆë¬¸ ì…ë ¥",
                    placeholder="PDFì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”...",
                    show_label=False,
                    scale=7
                )
                submit_button = gr.Button("ì „ì†¡", variant="primary", scale=1)
            clear_button = gr.Button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì§€ìš°ê¸°", variant="secondary", size="sm")

    # ì´ë²¤íŠ¸ ì—°ê²°
    upload_button.click(
        load_pdf,
        inputs=[file_input, chatbot],
        outputs=[chatbot, status_output]
    )
    
    # ì§ˆë¬¸ ì œì¶œ: 1) ì§ˆë¬¸ ì¦‰ì‹œ ì¶”ê°€ -> 2) ë‹µë³€ ìƒì„±
    msg.submit(
        add_message,
        inputs=[msg, chatbot],
        outputs=[chatbot, msg]
    ).then(
        chat_with_pdf,
        inputs=[chatbot],
        outputs=[chatbot]
    )
    
    submit_button.click(
        add_message,
        inputs=[msg, chatbot],
        outputs=[chatbot, msg]
    ).then(
        chat_with_pdf,
        inputs=[chatbot],
        outputs=[chatbot]
    )
    
    clear_button.click(
        clear_chat,
        outputs=[chatbot]
    )

demo.launch(show_error=True)

 